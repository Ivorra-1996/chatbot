# Chat IA (Streamlit + Ollama DeepSeekâ€‘R1)
**Docker + GPU (RTX 4060) + TÃºnel pÃºblico para embeber en Moodle**

PequeÃ±a app de chat hecha con **Streamlit** que consume **Ollama** (modelo `deepseek-r1:8b`). Incluye:
- **Docker Compose** con 3 servicios: `ollama`, `model-pull` (descarga el modelo) y `app` (Streamlit).
- Soporte **GPU** (probado con **GeForce RTX 4060 8 GB**).
- **Cloudflare Tunnel** para exponer la app con una **URL pÃºblica** y embeberla en el campus (Moodle).

---

## ğŸš€ Stack
- **Frontend**: Streamlit
- **LLM runtime**: Ollama (`deepseek-r1:8b`, quant Q4_K_M)
- **Infra**: Docker & Docker Compose
- **Opcional**: Cloudflare Tunnel para URL pÃºblica efÃ­mera

---

## ğŸ“ Estructura
```
tu-proyecto/
â”œâ”€ app/
â”‚  â”œâ”€ app.py
â”‚  â”œâ”€ requirements.txt
â”‚  â””â”€ .streamlit/
â”‚     â””â”€ config.toml
â”œâ”€ Dockerfile.app
â””â”€ docker-compose.yml
```

**Puntos clave**
- `app.py` usa `ollama.Client(host=OLLAMA_HOST)` para hablar con `ollama` en red interna.
- `config.toml` desactiva CORS/XSRF para permitir `<iframe>` (en producciÃ³n controlar con CSP en proxy).

---

## âœ… Requisitos
- **Windows 10/11** con **Docker Desktop (backend WSL2)**.
- **GPU activa en Docker**: Docker Desktop â†’ *Settings â†’ Resources â†’ GPU*.
- Drivers NVIDIA actualizados.

**Test rÃ¡pido de GPU en Docker**
```powershell
wsl --update
docker run --rm --gpus all nvidia/cuda:12.5.0-base-ubuntu22.04 nvidia-smi
```
Debe listar tu **RTX 4060**.

---

## âš™ï¸ ConfiguraciÃ³n de la app

### `app/.streamlit/config.toml`
```toml
[server]
headless = true
enableCORS = false
enableXsrfProtection = false
port = 8501
```

### Variables de entorno usadas
- `MODEL_NAME` (por defecto `deepseek-r1:8b`)
- `OLLAMA_HOST` (por defecto `http://ollama:11434` dentro de la red Docker)

---

## â–¶ï¸ Levantar todo (local)
```powershell
docker compose up --build -d
```

AbrÃ­: **http://localhost:8501**

> **Nota**: el servicio `model-pull` descarga el modelo al iniciar. Si ves 404 de modelo, ejecutÃ¡:  
> `docker exec -it ollama ollama pull deepseek-r1:8b`

---

## ğŸ§  Usar GPU
El servicio `ollama` ya tiene `gpus: all`. Verificaciones:

```powershell
# Â¿Ve la GPU dentro del contenedor?
docker exec -it ollama sh -lc 'ls /dev/nvidia* || true'

# Disparar una generaciÃ³n y ver logs (deberÃ­an mencionar CUDA/GPU)
curl -sS http://localhost:11434/api/generate -d '{ "model":"deepseek-r1:8b", "prompt":"hola" }' > NUL
docker logs -n 200 ollama | findstr /i cuda
```

Si no aparece GPU en logs:
- ConfirmÃ¡ GPU en Docker Desktop (Settings â†’ Resources â†’ GPU).
- ReiniciÃ¡ servicios: `docker compose down && docker compose up -d`.

---

## ğŸŒ URL pÃºblica (Cloudflare Tunnel)
El `docker-compose.yml` incluye un servicio `tunnel` que crea una URL efÃ­mera:

```powershell
docker compose up -d tunnel
docker logs -f cf-tunnel
```

BuscÃ¡ la lÃ­nea con `https://<algo>.trycloudflare.com`. Esa es tu **URL pÃºblica**.

> La URL cambia si reiniciÃ¡s el tÃºnel. Para una URL fija (p. ej., `https://chat.tu-dominio.edu.ar`), usar **Named Tunnel** (se configura luego).

---

## ğŸ§© Embeber en Moodle

**OpciÃ³n A (recomendada):** *Recurso â†’ URL*  
- Pegar la URL pÃºblica (del tÃºnel).
- **Apariencia**: _Embed_ / _En marco_.

**OpciÃ³n B:** *Recurso â†’ PÃ¡gina/Etiqueta* (modo HTML)  
```html
<iframe
  src="https://<tu-url>.trycloudflare.com"
  width="100%"
  height="720"
  style="border:1px solid #ddd;border-radius:12px;overflow:hidden"
  allow="clipboard-read; clipboard-write">
</iframe>
```

> Si el iframe queda en blanco por polÃ­ticas de seguridad (*frame-ancestors*), en producciÃ³n agregÃ¡ en tu proxy (Nginx/Caddy/Cloudflare) un **CSP** con el dominio del campus:  
> `Content-Security-Policy: frame-ancestors 'self' https://campus.tuuni.edu.ar`

---

## ğŸ”§ Troubleshooting

**`model "deepseek-r1:8b" not found (404)`**
```powershell
docker exec -it ollama ollama list
docker exec -it ollama ollama pull deepseek-r1:8b
```

**La app no llega a Ollama**
```powershell
docker exec -it streamlit-app sh -lc 'printenv OLLAMA_HOST && curl -sS $OLLAMA_HOST/api/tags'
```
Debe apuntar a `http://ollama:11434` y listar el modelo.

**GPU no usada**
- Ver `gpus: all` en `ollama`.
- Docker Desktop con GPU habilitada.
- Logs de `ollama` deberÃ­an mencionar CUDA tras una respuesta.

**URL pÃºblica no carga**
- EsperÃ¡ unos segundos tras crear el tÃºnel.
- RevisÃ¡ `docker logs -f cf-tunnel`.
- VerificÃ¡ que `app` estÃ© arriba: `docker logs -f streamlit-app`.

---

## ğŸ§± Seguridad
- **No** expongas el puerto de **Ollama** a Internet; sÃ³lo la app.
- Para producciÃ³n: TLS, dominio propio y **CSP `frame-ancestors`** para limitar quiÃ©n puede embeber.

---

## ğŸ“œ Comandos Ãºtiles

```powershell
# Logs
docker compose logs -f app
docker compose logs -f ollama
docker compose logs -f cf-tunnel

# Reconstruir
docker compose down
docker compose up --build -d

# Borrar modelos (Â¡destruye la cachÃ©!)
docker stop ollama && docker rm ollama
docker volume rm tu-proyecto_ollama
```
